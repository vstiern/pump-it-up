{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven: Pump It Up (2/3)\n",
    "\n",
    "**Vilhelm Stiernstedt & Camillo Baratta**\n",
    "<br>\n",
    "** Date: 14/03/2018**\n",
    "\n",
    "** Instructions **\n",
    "<br>\n",
    "Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "<br>\n",
    "<br>\n",
    "** Goal **\n",
    "<br>\n",
    "Your goal is to predict the operating condition of a waterpoint for each record in the dataset.\n",
    "The labels in this dataset are simple. There are three possible values:\n",
    "\n",
    "- functional - the waterpoint is operational and there are no repairs needed\n",
    "- functional needs repair - the waterpoint is operational, but needs repairs\n",
    "- non functional - the waterpoint is not operational\n",
    "\n",
    "**Plan**\n",
    "1. Import libraries\n",
    "2. Import data\n",
    "3. Data description\n",
    "4. Feature analysis\n",
    "5. Data cleaning\n",
    "    - Impute missing values\n",
    "    - Outlier treatment\n",
    "6. Feature engineering\n",
    "    - Transformation\n",
    "    - Binning\n",
    "    - New features\n",
    "7. Feature selection\n",
    "    - Drop irrelevant features\n",
    "8. Modelling Preparation\n",
    "    - One-hot encoding\n",
    "    - Test/train split\n",
    "9. Modelling\n",
    "    - Randomforest model\n",
    "    - Light GBM\n",
    "    - KNN\n",
    "    - LDA\n",
    "    - LR-OVR (OneVsAll)\n",
    "    - Stacked Model\n",
    "10. Submission\n",
    "\n",
    "<br>\n",
    "** Notebooks used in project:**\n",
    "<br>\n",
    "The different steps are distrubuted over 3 notebooks:\n",
    "1. Data Analysis - step 1-4 (Pump_Data_Analysis)\n",
    "2. Data Preparation - steps 5-8 (Pump_Data_Preparation)\n",
    "3. Data Modelling - steps 9-10 (Pump_Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Plot settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "1. Load csv files:\n",
    "    - train_values\n",
    "    - train_labels\n",
    "    - test_values\n",
    "2. Merge train_values and train_labels into **train** - use this df for all data analysis.\n",
    "3. Merge train with test_values into **all_data** - use this df for all cleaning and then split train and test before modelling.\n",
    "3. Write files to csv for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# Import data\n",
    "train_values = pd.read_csv(\"data/train_set_values.csv\", sep=\",\")\n",
    "train_labels = pd.read_csv(\"data/train_set_labels.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"data/test.csv\", sep=\",\")\n",
    "\n",
    "# Merge train with train_labels\n",
    "train = train_values.merge(train_labels, on='id', how='inner')\n",
    "\n",
    "# Merge train with test_values\n",
    "all_data = train.append(test)\n",
    "\n",
    "# Set  id index\n",
    "all_data.set_index('id', inplace=True)\n",
    "train_labels.set_index('id', inplace=True)\n",
    "train.set_index('id', inplace=True)\n",
    "test.set_index('id', inplace=True)\n",
    "\n",
    "# Write to csv\n",
    "#train.to_csv('data/train.csv', sep=',')\n",
    "#test.to_csv('data/test.csv', sep=',')\n",
    "#all_data.to_csv('data/all_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data types\n",
    "date_recorded -> string to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dtypes\n",
    "# Numerical to datetime (Year-MM-DD)\n",
    "all_data.date_recorded = pd.to_datetime(all_data.date_recorded,\n",
    "                                        errors='ignore',\n",
    "                                        format='%Y-%m-%d')\n",
    "\n",
    "train.date_recorded = pd.to_datetime(train.date_recorded,\n",
    "                                     errors='ignore',\n",
    "                                     format='%Y-%m-%d')\n",
    "\n",
    "test.date_recorded = pd.to_datetime(test.date_recorded,\n",
    "                                    errors='ignore',\n",
    "                                    format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning \n",
    "Go through each group of features and perform the following operations:\n",
    "1. Missing values imputation\n",
    "2. Outlier treatment\n",
    "\n",
    "### Missing values imputations\n",
    "All imputation is done with df: all_data\n",
    "\n",
    "1. NaN imputation\n",
    "2. Incorrect zero values\n",
    "\n",
    "Note: Prior to 2. the data was analysed thoroughly to define which zero values needed to be treated and which not.\n",
    "\n",
    "### NaN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "funder                4504\n",
       "installer             4532\n",
       "permit                3793\n",
       "public_meeting        4155\n",
       "scheme_management     4846\n",
       "scheme_name          35258\n",
       "subvillage             470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nan = all_data.drop(['status_group'], axis=1).isnull().sum()\n",
    "all_nan[all_nan > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 variables were found to have null-values. scheme_name had the most by far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subvillage\n",
    "\n",
    "1. Impute all NaNs to 'unknown'\n",
    "2. Impute 'unknown' to all subvillages with len(str) < 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.subvillage.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.subvillage.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nr of longitude equal zero again\n",
    "all_data.loc[(all_data.subvillage.str.len()) < 3, 'subvillage'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute subvillages with string len of 1 to unknown\n",
    "all_data.loc[(all_data.subvillage.str.len()) < 3, 'subvillage'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that no remains\n",
    "all_data[(all_data.subvillage.str.len()) < 3].subvillage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scheme_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.scheme_name.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.scheme_name.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scheme_management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4846"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.scheme_management.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.scheme_management.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4532"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.installer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.installer.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4504"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.funder.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.funder.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### public_meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4155"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.public_meeting.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.public_meeting.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### permit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3793"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.permit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with False\n",
    "all_data.permit.fillna(value='False', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scheme_management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaNs count\n",
    "all_data.scheme_management.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with unknown\n",
    "all_data.scheme_management.fillna(value='Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that all NaNs are zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that all NaNs are zero \n",
    "all_nan = all_data.drop(['status_group'], axis=1).isnull().sum()\n",
    "all_nan[all_nan > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero-value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### longitude\n",
    "\n",
    "1. Convert zeros to NaNs.\n",
    "2. Impute NaNs longitude per subvillage mean\n",
    "3. For remaning, impute longitude per ward mean\n",
    "4. For remaning, impute longitude per lga mean\n",
    "\n",
    "Note: we chose the approach to gain the most useful imputation without getting lost in too detailed data (e.g. lga). this approach was based on our analysis of the data (i.e. understanding the meaning of lga, ward, subvillage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2269"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nr of longitude equal zero\n",
    "all_data.loc[all_data.longitude == 0, 'longitude'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all zeros to NaN\n",
    "all_data.loc[all_data.longitude == 0, 'longitude'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2269"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaNs\n",
    "all_data.longitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs longitude per subvillage mean\n",
    "all_data.longitude.fillna(all_data.groupby('subvillage').longitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1410"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaNs\n",
    "all_data.longitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs longitude per ward mean\n",
    "all_data.longitude.fillna(all_data.groupby('ward').longitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaNs\n",
    "all_data.longitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs longitude per lga mean\n",
    "all_data.longitude.fillna(all_data.groupby('lga').longitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for NaNs\n",
    "all_data.longitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    74250.000000\n",
       "mean        35.121277\n",
       "std          2.580671\n",
       "min         29.607122\n",
       "25%         33.303138\n",
       "50%         34.945507\n",
       "75%         37.184269\n",
       "max         40.345193\n",
       "Name: longitude, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall info\n",
    "all_data.longitude.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f729ffa891f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot new distrubition of longitude  -> looks good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# plot new distrubition of longitude  -> looks good\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(all_data.longitude);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### latitude\n",
    "\n",
    "1. Convert zeros to NaNs.\n",
    "2. Impute NaNs latitude per subvillage mean\n",
    "3. For remaning, impute latitude per ward mean\n",
    "4. For remaning, impute latitude per lga mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nr of latitude equal zero\n",
    "all_data.loc[all_data.latitude > -1.1, 'latitude'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all zeros to NaN\n",
    "all_data.loc[all_data.latitude > -1.1, 'latitude'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.latitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs latitude per subvillage mean\n",
    "all_data.latitude.fillna(all_data.groupby('subvillage').latitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.latitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs latitude per ward mean\n",
    "all_data.latitude.fillna(all_data.groupby('ward').latitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.latitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs latitude per lga mean\n",
    "all_data.latitude.fillna(all_data.groupby('lga').latitude.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.latitude.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall info\n",
    "all_data.latitude.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new distrubition of latitude  -> looks good\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(all_data.latitude);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gps_height\n",
    "\n",
    "1. Convert zeros to NaNs\n",
    "2. Concat latitude and longitude to create new variable latlong\n",
    "3. Use gps_height mean for latlong \n",
    "4. For remaining use, use gps_height mean for subvillage\n",
    "5. For remaining use, use gps_height mean for ward\n",
    "6. For remaining use, use gps_height mean for lga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero values\n",
    "all_data[all_data.gps_height == 0].gps_height.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all zeros to NaN\n",
    "all_data.loc[all_data.gps_height == 0, 'gps_height'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gps_height\n",
    "# Merge latitude and longitude into new variable, using only 2 decimals\n",
    "all_data['latlong'] = all_data.latitude.round(2).map(str) + \"-\" + all_data.longitude.round(2).map(str)\n",
    "all_data.latlong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs gps_height per latlong mean\n",
    "all_data.gps_height.fillna(all_data.groupby('latlong').gps_height.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs -> just fill anything, too granular\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs gps_height per subvillage mean\n",
    "all_data.gps_height.fillna(all_data.groupby('subvillage').gps_height.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs gps_height per ward mean\n",
    "all_data.gps_height.fillna(all_data.groupby('ward').gps_height.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs gps_height per lga mean\n",
    "all_data.gps_height.fillna(all_data.groupby('lga').gps_height.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs gps_height per region median\n",
    "all_data.gps_height.fillna(all_data.groupby('region').gps_height.transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.gps_height.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.gps_height.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new distrubition of gps_height -> looks good\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(all_data.gps_height);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction_year\n",
    "We assume that water points have been constructed in areas around similiar times by different scheme managements. Thus we will use the following strategy:\n",
    "\n",
    "1. Convert all values below 1960 to NaNs\n",
    "2. Impute all NaNs by taking the mean subvillage and scheme_management construction year.\n",
    "3. For remaining, impute by taking the mean ward and scheme_management construction year.\n",
    "4. For remaining, mpute by taking the mean lga and scheme_management construction year.\n",
    "5. For remaining, impute by taking the mean region and scheme_management construction year.\n",
    "6. For remaining, impute by taking the only mean scheme_management construction year.\n",
    "7. For remaining, impute by taking the only mean subvillage construction year.\n",
    "8. For remaining, impute by taking the only mean ward construction year.\n",
    "9. For remaining, impute by taking the only mean lga construction year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nr of construction years less than 1960 \n",
    "all_data.loc[all_data.construction_year < 1960, 'construction_year'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all zeros to NaN\n",
    "all_data.loc[all_data.construction_year < 1960, 'construction_year'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per subvillage and scheme_management mean\n",
    "all_data.construction_year.fillna(all_data.groupby(['subvillage', 'scheme_management'])\\\n",
    "                                  .construction_year.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per ward and scheme_management mean\n",
    "all_data.construction_year.fillna(all_data.groupby(['ward', 'scheme_management'])\\\n",
    "                                  .construction_year.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per lga and scheme_management mean\n",
    "all_data.construction_year.fillna(all_data.groupby(['lga', 'scheme_management'])\\\n",
    "                                  .construction_year.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per region and scheme_management mean\n",
    "all_data.construction_year.fillna(all_data.groupby(['region', 'scheme_management'])\\\n",
    "                                  .construction_year.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per scheme_management mean\n",
    "all_data.construction_year.fillna(all_data.groupby(['scheme_management'])\\\n",
    "                                  .construction_year.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.construction_year.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.construction_year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new distrubition of construction_year -> looks okay\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(all_data.construction_year);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if there is any constructions years that are newer than date_recorded\n",
    "all_data[(all_data.date_recorded.dt.year - all_data.construction_year) < 0].construction_year.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimpute these records for constuction date -> date_recorded - 1\n",
    "all_data.loc[(all_data.date_recorded.dt.year - all_data.construction_year) < 0,\n",
    "             'construction_year'] = all_data.date_recorded.dt.year - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### population\n",
    "\n",
    "1. Convert zeros to NaNs\n",
    "2. Impute use population mean for subvillage\n",
    "3. For remaining use mean for ward\n",
    "4. For remaining use mean for lga\n",
    "5. For remaining use mean for region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nr of population equal zero\n",
    "all_data.loc[all_data.population == 0, 'population'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all zeros to NaN\n",
    "all_data.loc[all_data.population == 0, 'population'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.population.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per subvillage\n",
    "all_data.population.fillna(all_data.groupby(['subvillage']).population.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.population.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per ward\n",
    "all_data.population.fillna(all_data.groupby(['ward']).population.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.population.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per lga\n",
    "all_data.population.fillna(all_data.groupby(['lga']).population.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.population.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs construction_year per region\n",
    "all_data.population.fillna(all_data.groupby(['region']).population.transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaNs\n",
    "all_data.population.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.population.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new distrubition of populaton in log -> looks bad -> need log transformation!\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(np.log10(all_data.population + 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pop = np.log10(all_data.population + 1)\n",
    "log_pop[log_pop < 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis\n",
    "Here our outlier analysis and treatment goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data back to train and test from all_data\n",
    "train = all_data[:len(train)]\n",
    "test = all_data[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.quantity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amout_tsh equal zero but well is functional and has enough water -> impute mean for population\n",
    "train[(train.amount_tsh == 0) &\n",
    "      (train.status_group == 'functional') &\n",
    "      (train.quantity == 'enough')].amount_tsh.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to NaN first \n",
    "all_data.loc[(all_data.amount_tsh == 0) &\n",
    "      (all_data.status_group == 'functional') &\n",
    "      (all_data.quantity == 'enough'), 'amount_tsh'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs amount_tsh per subvillage and population\n",
    "all_data.amount_tsh.fillna(all_data.groupby(['subvillage', 'population']).amount_tsh.transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.amount_tsh.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs amount_tsh per subvillage with same conditions\n",
    "all_data.amount_tsh.fillna(all_data.groupby(['subvillage']).amount_tsh.transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.amount_tsh.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs amount_tsh per subvillage with same conditions\n",
    "all_data.amount_tsh.fillna(all_data.groupby(['population']).amount_tsh.transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.amount_tsh.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaNs amount_tsh per subvillage with same conditions\n",
    "all_data.amount_tsh.fillna(all_data.groupby(['ward']).amount_tsh.transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.amount_tsh.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "1. Transformations\n",
    "2. Binning\n",
    "3. New features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "#### Log\n",
    "1. amount_tsh\n",
    "2. population\n",
    "\n",
    "We decided to use a log transformation because unbalanced distrubution of features, i.e. try to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable: amount_tsh_log\n",
    "all_data['amount_tsh_log'] = np.log10(all_data.amount_tsh + 1)\n",
    "\n",
    "# Create new variable: population_log\n",
    "all_data['population_log'] = np.log10(all_data.population + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Binnning\n",
    "Reclassify low value count levels to unknown / other:\n",
    "1. management_group \n",
    "2. quality_group\n",
    "3. extraction_type_class\n",
    "4. scheme_management\n",
    "\n",
    "First, create new variable with only 5 first chars, then Bin levels depening on value count:\n",
    "1. funder\n",
    "2. installer\n",
    "3. subvillage\n",
    "4. ward\n",
    "5. lga\n",
    "6. region\n",
    "7. wpt_name\n",
    "\n",
    "Binary binning:\n",
    "1. wpt_name -> 2 levels: name, no name\n",
    "2. scheme_name -> 2 levels: name, no name\n",
    "3. payment_type -> 2 levels: pay, never_pay\n",
    "4. payment_type_fixed -> 2 levels: fixed, not fixed\n",
    "5. num_private -> 2 levels: number, no number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_table = {'management_group': {'other': 'unknown'},\n",
    "\n",
    "                    'quality_group': {'fluoride': 'unknown',\n",
    "                                       'colored': 'unknown',\n",
    "                                      },\n",
    "\n",
    "                    'extraction_type_class': {'rope pump': 'other',\n",
    "                                             'wind-powered': 'other'\n",
    "                                             },\n",
    "\n",
    "                    'scheme_management': {'None': 'Other',\n",
    "                                          'Trust': 'Other',\n",
    "                                          'SWC': 'Other'\n",
    "                                         }}\n",
    "\n",
    "all_data = all_data.replace(binning_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variables with only five first characters\n",
    "all_data['funder_5char'] = all_data.funder.str[:5]\n",
    "all_data['installer_5char'] = all_data.installer.str[:5]\n",
    "\n",
    "# Define function to bin value depending on value count\n",
    "def binning_cat(x):\n",
    "    if x <= 1:\n",
    "        return 's1'\n",
    "    elif (x>1) & (x<=10):\n",
    "        return 's10'\n",
    "    elif (x>10) & (x<=25):\n",
    "        return 's25'\n",
    "    elif (x>25) & (x<=50):\n",
    "        return 's50'\n",
    "    elif (x>50) & (x<=100):\n",
    "        return 's100'\n",
    "    elif (x>100) & (x<=250):\n",
    "        return 's250'\n",
    "    elif (x>250) & (x<=500):\n",
    "        return 's500'\n",
    "    elif (x>500) & (x<=1000):\n",
    "        return 's1000'\n",
    "    elif (x>1000):\n",
    "        return 's+1000'\n",
    "\n",
    "# funder_5char\n",
    "# Create pivot_table with value count for funder_5char\n",
    "funder_piv = all_data.pivot_table(index='funder_5char', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "funder_df = pd.DataFrame()\n",
    "funder_df['type'] = funder_piv.index\n",
    "funder_df['count'] = funder_piv.values\n",
    "funder_df['funder_size'] = \\\n",
    "    funder_df[funder_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "funder_df.loc[funder_df.type == 'Unkno', 'funder_size'] = 'unknown'\n",
    "funder_map = dict(funder_df[['type', 'funder_size']].values)\n",
    "\n",
    "# Create new variable funder_size based on binning\n",
    "all_data['funder_size'] = all_data.funder_5char.map(funder_map)\n",
    "\n",
    "\n",
    "# installer_5char\n",
    "# Create pivot_table with value count for installer_5char\n",
    "installer_piv = all_data.pivot_table(index='installer_5char', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "installer_df = pd.DataFrame()\n",
    "installer_df['type'] = installer_piv.index\n",
    "installer_df['count'] = installer_piv.values\n",
    "installer_df['installer_size'] = \\\n",
    "    installer_df[installer_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "installer_df.loc[installer_df.type == 'Unkno', 'installer_size'] = 'unknown'\n",
    "installer_map = dict(installer_df[['type', 'installer_size']].values)\n",
    "\n",
    "# Create new variable installer_size based on binning\n",
    "all_data['installer_size'] = all_data.installer_5char.map(installer_map)\n",
    "\n",
    "\n",
    "# subvillage\n",
    "# Create pivot_table with value count for subvillage\n",
    "subvillage_piv = all_data.pivot_table(index='subvillage', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "subvillage_df = pd.DataFrame()\n",
    "subvillage_df['type'] = subvillage_piv.index\n",
    "subvillage_df['count'] = subvillage_piv.values\n",
    "subvillage_df['subvillage_size'] = \\\n",
    "    subvillage_df[subvillage_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "subvillage_df.loc[subvillage_df.type == 'Unkno', 'subvillage_size'] = 'unknown'\n",
    "subvillage_map = dict(subvillage_df[['type', 'subvillage_size']].values)\n",
    "\n",
    "# Create new variable subvillage_size based on binning\n",
    "all_data['subvillage_size'] = all_data.subvillage.map(subvillage_map)\n",
    "\n",
    "\n",
    "# ward\n",
    "# Create pivot_table with value count for ward\n",
    "ward_piv = all_data.pivot_table(index='ward', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "ward_df = pd.DataFrame()\n",
    "ward_df['type'] = ward_piv.index\n",
    "ward_df['count'] = ward_piv.values\n",
    "ward_df['ward_size'] = \\\n",
    "    ward_df[ward_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "ward_df.loc[ward_df.type == 'Unkno', 'ward_size'] = 'unknown'\n",
    "ward_map = dict(ward_df[['type', 'ward_size']].values)\n",
    "\n",
    "# Create new variable ward_size based on binning\n",
    "all_data['ward_size'] = all_data.ward.map(ward_map)\n",
    "\n",
    "\n",
    "# lga\n",
    "# Create pivot_table with value count for lga\n",
    "lga_piv = all_data.pivot_table(index='lga', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "lga_df = pd.DataFrame()\n",
    "lga_df['type'] = lga_piv.index\n",
    "lga_df['count'] = lga_piv.values\n",
    "lga_df['lga_size'] = \\\n",
    "    lga_df[lga_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "lga_df.loc[lga_df.type == 'Unkno', 'lga_size'] = 'unknown'\n",
    "lga_map = dict(lga_df[['type', 'lga_size']].values)\n",
    "\n",
    "# Create new variable lga_size based on binning\n",
    "all_data['lga_size'] = all_data.lga.map(lga_map)\n",
    "\n",
    "\n",
    "# wpt_name\n",
    "# Create pivot_table with value count for wpt_name\n",
    "wpt_name_piv = all_data.pivot_table(index='wpt_name', values='quantity', aggfunc=len)\n",
    "\n",
    "# Create datatable to populate with pivot table data along with binning values\n",
    "# Keep unknown records as unknown\n",
    "wpt_name_df = pd.DataFrame()\n",
    "wpt_name_df['type'] = wpt_name_piv.index\n",
    "wpt_name_df['count'] = wpt_name_piv.values\n",
    "wpt_name_df['wpt_name_size'] = \\\n",
    "    wpt_name_df[wpt_name_df.type != 'Unkno']['count'].apply(lambda x: binning_cat(x))\n",
    "wpt_name_df.loc[wpt_name_df.type == 'Unkno', 'wpt_name_size'] = 'unknown'\n",
    "wpt_name_map = dict(wpt_name_df[['type', 'wpt_name_size']].values)\n",
    "\n",
    "# Create new variable wpt_name_size based on binning\n",
    "all_data['wpt_name_size'] = all_data.wpt_name.map(wpt_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary binning:\n",
    "# 1. wpt_name -> 2 levels: name, no name\n",
    "all_data['wpt_name_bin'] = all_data['wpt_name']\n",
    "all_data.loc[(all_data.wpt_name_bin != 'none'), 'wpt_name_bin'] = 'name'\n",
    "\n",
    "# 2. scheme_name -> 2 levels: name, no name\n",
    "all_data['scheme_name_bin'] = all_data['scheme_name']\n",
    "all_data.loc[(all_data.scheme_name_bin != 'None'), 'scheme_name_bin'] = 'Name'\n",
    "\n",
    "# 3. payment_type -> 2 levels: pay, never_pay\n",
    "all_data['payment_type_bin'] = all_data['payment_type']\n",
    "all_data.loc[(all_data.payment_type_bin != 'never pay'), 'payment_type_bin'] = 'pay'\n",
    "\n",
    "# 4. payment_type_fixed -> 2 levels: fixed, not fixed\n",
    "all_data['payment_type_fixed'] = all_data['payment_type']\n",
    "all_data.loc[(all_data.payment_type_fixed == 'monthly') | (all_data.payment_type_fixed == 'annually'),\n",
    "             'payment_type_fixed'] = 'fixed'\n",
    "all_data.loc[(all_data.payment_type_fixed != 'fixed'), 'payment_type_fixed'] = 'not fixed'\n",
    "\n",
    "# 5. num_private -> 2 levels: number, no number\n",
    "all_data['num_private_bin'] = all_data['num_private']\n",
    "all_data.loc[(all_data.num_private_bin != 0), 'num_private_bin'] = 'number'\n",
    "all_data.loc[(all_data.num_private_bin != 'number'), 'num_private_bin'] = 'no number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Funder binning\n",
    "all_data['funder_simple'] = all_data['funder'].str.lower()\n",
    "# foregin gov\n",
    "all_data['funder_simple'].replace(['danida', 'a/co germany', 'belgian', 'british', 'england', 'german', 'germany',\n",
    "         'china', 'egypt', 'european union', 'finland', 'japan', 'france', 'greec',\n",
    "         'netherlands', 'holland', 'holand', 'nethe', 'nethalan', 'netherla', 'netherlands',\n",
    "         'iran', 'irish', 'islam','italy', 'u.s.a', 'usa', 'usaid', 'swiss', 'swedish','korea', 'niger',\n",
    "                                   'germany republi'], 'f_gov', inplace=True)\n",
    "\n",
    "# NGOs\n",
    "all_data['funder_simple'].replace(['world bank', 'ngo', \"ngos\", \"un\",\"un hhabitat\", \"un/wfp\", \"undp\", \"undp/aict\", \"undp/ilo\", \"unesco\",                        \n",
    "       \"unhcr\", \"unhcr/government\", \"unice\", \"unice/ cspd\", \"unicef\", \"unicef/ csp\", \"unicef/african muslim agency\", \n",
    "       \"unicef/central\", \"unicef/cspd\", \"uniceg\", \"unicet\", \"unicrf\", \"uniseg\", \"unp/aict\", \"wwf\", \"wfp\", \"oxfam\",\n",
    "                                   \"world vision\", \"rc church\", \"wateraid\", ], 'ngo', inplace=True)\n",
    "\n",
    "# local government\n",
    "all_data['funder_simple'].replace(['government of tanzania', 'district council', 'ministry of water'],\n",
    "                                  'l_gov', inplace=True)\n",
    "\n",
    "# others\n",
    "all_data.loc[(all_data.funder_simple != 'f_gov') &\n",
    "             (all_data.funder_simple != 'ngo') &\n",
    "             (all_data.funder_simple != 'l_gov'), 'funder_simple'] = 'unknown'\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Variables\n",
    "Create the following new variables:\n",
    "1. recored_year -> YY from recorded_date\n",
    "2. recored_month -> MM from recorded_date\n",
    "3. age -> construction year - recored_year\n",
    "4. latlong -> round(latitude + longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recored_date -> YY\n",
    "all_data['recorded_year'] = all_data.date_recorded.dt.year\n",
    "\n",
    "# Recored_date -> MM\n",
    "all_data['recorded_month'] = all_data.date_recorded.dt.month\n",
    "\n",
    "# Age = Recored_date - construction_year\n",
    "all_data['well_age'] = all_data.recorded_year-all_data.construction_year\n",
    "\n",
    "# Construction decade\n",
    "all_data['construction_decade'] = all_data.construction_year\n",
    "all_data.loc[(all_data.construction_decade > 1959) & (all_data.construction_decade < 1970),\n",
    "             'construction_decade'] = 1960\n",
    "all_data.loc[(all_data.construction_decade > 1969) & (all_data.construction_decade < 1980),\n",
    "             'construction_decade'] = 1970\n",
    "all_data.loc[(all_data.construction_decade > 1979) & (all_data.construction_decade < 1990),\n",
    "             'construction_decade'] = 1980\n",
    "all_data.loc[(all_data.construction_decade > 1989) & (all_data.construction_decade < 2000),\n",
    "             'construction_decade'] = 1990\n",
    "all_data.loc[(all_data.construction_decade > 1999) & (all_data.construction_decade < 2010),\n",
    "             'construction_decade'] = 2000\n",
    "all_data.loc[(all_data.construction_decade > 2009),\n",
    "             'construction_decade'] = 2010\n",
    "\n",
    "# Simple Latitude \n",
    "all_data['lat_simple'] = round(all_data.latitude, 1)\n",
    "\n",
    "# Simple Longitude\n",
    "all_data['long_simple'] = round(all_data.longitude, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature selection\n",
    "\n",
    "Drop the following features:\n",
    "1. num_private -> replaced by engineered feature\n",
    "2. payment -> same as payment_type\n",
    "3. extraction_type -> same as extraction_class_group\n",
    "4. extraction_type_group -> same as extraction_class_group\n",
    "5. quantity_group -> same as quantity\n",
    "6. water_quality -> same as quality_group but more levels\n",
    "7. source -> same as source_type but with unknowns\n",
    "8. source_class -> less granular than source_type, only 3 levels\n",
    "9. waterpoint_type -> same as waterpoint_type_groupe but with more levels\n",
    "10. recorded_by -> only one level\n",
    "11. management -> same as scheme_management but messier text\n",
    "12. region_code -> prefer names\n",
    "13. district_code -> prefer names\n",
    "14. lga -> replaced by engineered feature\n",
    "15. ward -> replaced by engineered feature\n",
    "16. subvillage -> replaced by engineered feature\n",
    "17. funder -> replaced by engineered feature\n",
    "18. installer -> replaced by engineered feature\n",
    "19. scheme_name -> replaced by engineered feature\n",
    "20. wpt_name -> replaced by engineered feature\n",
    "21. funder_5char -> replaced by engineered feature\n",
    "22. installer_5char -> replaced by engineered feature\n",
    "23. construction_year -> replaced by engineered feature\n",
    "24. date_recorded -> replaced by engineered feature\n",
    "25. population -> replaced by engineered feature\n",
    "26. latitude -> replaced by engineered feature\n",
    "27. longitude -> replaced by engineered feature\n",
    "28. amount_tsh -> replaced by engineered feature\n",
    "29. latlong -> too granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to drop\n",
    "drop_features = ['num_private', 'payment', 'extraction_type', 'extraction_type_group', 'quantity_group',\n",
    "                 'water_quality', 'source', 'source_class', 'waterpoint_type', 'recorded_by', 'management',\n",
    "                 'region_code', 'district_code', 'lga', 'ward', 'subvillage', 'funder', 'installer', 'scheme_name',\n",
    "                 'wpt_name', 'funder_5char', 'installer_5char', 'construction_year', 'date_recorded', 'population',\n",
    "                 'latitude', 'longitude', 'amount_tsh', 'latlong']\n",
    "\n",
    "# Drop features\n",
    "all_data.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "# Remaining nummerical features\n",
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining catergorical features\n",
    "all_cat_vars = all_data.select_dtypes(include=['object'])\n",
    "all_cat_vars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Resampling (not used atm)\n",
    "Given there is inbalance of the three different targets, need repairs only represent 7% of data, could it improve to resample some of the data and expand the training set to achieve better predictions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count majority and minority classes\n",
    "functional_count = all_data[all_data.status_group == 'functional'].status_group.count()\n",
    "non_functional_count = all_data[all_data.status_group == 'non functional'].status_group.count()\n",
    "repairs_count = all_data[all_data.status_group == 'functional needs repair'].status_group.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = all_data[all_data.status_group == 'functional']\n",
    "non_functional_minority = all_data[all_data.status_group == 'non functional']\n",
    "repairs_minority = all_data[all_data.status_group == 'functional needs repair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "non_functional_upsampled = resample(non_functional_minority,\n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=functional_count,    # to match majority class\n",
    "                                 random_state=42) # reproducible results\n",
    "\n",
    "reparis_upsampled = resample(repairs_minority,\n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=functional_count,    # to match majority class\n",
    "                                 random_state=42) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "new_data = pd.concat([df_majority, non_functional_upsampled, reparis_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "new_data.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modelling Prepartation\n",
    "\n",
    "1. Drop status_group from all_data\n",
    "2. One-hot encoding - Create dummy variables for all categorical features.\n",
    "3. Split all_data back into train_data and test_data\n",
    "4. Factorize training labels -> numerical representation\n",
    "5. Write clean data to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop status_group\n",
    "all_data.drop('status_group', axis=1, inplace=True)\n",
    "\n",
    "# One-hot encoding: Create dummies for all_data\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# Split data back to train and test from all_data (split based on empty status_group)\n",
    "train_data = all_data[:len(train)]\n",
    "test_data = all_data[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize train_labels\n",
    "labels = pd.factorize(train_labels.status_group)[0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned data to csv\n",
    "train_data.to_csv('data/train_clean.csv', sep=',')\n",
    "test_data.to_csv('data/test_clean.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
